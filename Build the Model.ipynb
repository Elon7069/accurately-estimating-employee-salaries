from sklearn.model_selection import train_test_split

# Define feature columns (X) and target column (y)
X = data.drop('salary', axis=1)  # Replace 'salary' with your target column name
y = data['salary']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Dataset split into training and testing sets successfully!")


from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Select categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Combine preprocessors in a column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ]
)

print("Preprocessing pipeline created successfully!")

from sklearn.ensemble import RandomForestRegressor

# Define the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

print("Model defined successfully!")

from sklearn.pipeline import Pipeline

# Create the pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

print("Pipeline created successfully!")

# Train the pipeline
pipeline.fit(X_train, y_train)
print("Model trained successfully!")


from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Model Evaluation:\nMAE: {mae:.2f}\nRMSE: {rmse:.2f}")

from sklearn.model_selection import GridSearchCV

# Define hyperparameter grid
param_grid = {
    'model__n_estimators': [50, 100, 200],
    'model__max_depth': [None, 10, 20, 30]
}

# Perform grid search
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and score
print("Best parameters:", grid_search.best_params_)
print("Best MAE score:", -grid_search.best_score_)

import matplotlib.pyplot as plt
import pandas as pd

# Extract feature importances
feature_importances = grid_search.best_estimator_['model'].feature_importances_
feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out()

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': list(numerical_cols) + list(feature_names),
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.gca().invert_yaxis()
plt.show()

import joblib

# Save the trained pipeline
joblib.dump(grid_search.best_estimator_, 'salary_prediction_model.pkl')
print("Model saved as 'salary_prediction_model.pkl'!")








